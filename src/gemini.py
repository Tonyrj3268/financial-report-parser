import base64
import json
import os
import tempfile
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from contextlib import contextmanager
from pathlib import Path

import fitz
from dotenv import load_dotenv
from google import genai
from openpyxl import load_workbook
from pydantic import BaseModel, Field

from models.cash_equivalents import CashAndEquivalents, cash_equivalents_prompt
from models.prepayments import PrePayments, prepayments_prompt
from models.receivables_related_parties import (
    ReceivablesRelatedParties,
    receivables_related_parties_prompt,
)
from models.total_liabilities import TotalLiabilities, total_liabilities_prompt
from models.corporate_bond_payable import (
    CorporateBondPayable,
    corporate_bond_payable_prompt,
)
from models.property_plant_equipment import (
    PropertyPlantEquipment,
    property_plant_equipment_prompt,
)
from models.short_term_notes import (
    ShortTermNotesPayable,
    short_term_notes_payable_prompt,
)
from src.utils import call_gemini


# Tokenä½¿ç”¨è¨˜éŒ„å™¨
class TokenUsageTracker:
    """Tokenä½¿ç”¨è¨˜éŒ„å™¨"""

    def __init__(self):
        self.total_input_tokens = 0
        self.total_output_tokens = 0
        self.api_calls = 0
        self.call_details = []
        self._lock = threading.Lock()

    def add_usage(
        self, input_tokens: int, output_tokens: int, call_type: str = "unknown"
    ):
        """æ·»åŠ tokenä½¿ç”¨è¨˜éŒ„"""
        with self._lock:
            self.total_input_tokens += input_tokens
            self.total_output_tokens += output_tokens
            self.api_calls += 1
            self.call_details.append(
                {
                    "call_type": call_type,
                    "input_tokens": input_tokens,
                    "output_tokens": output_tokens,
                    "total_tokens": input_tokens + output_tokens,
                }
            )

    def get_summary(self) -> dict:
        """ç²å–ä½¿ç”¨æ‘˜è¦"""
        total_tokens = self.total_input_tokens + self.total_output_tokens
        return {
            "total_api_calls": self.api_calls,
            "total_input_tokens": self.total_input_tokens,
            "total_output_tokens": self.total_output_tokens,
            "total_tokens": total_tokens,
            "call_details": self.call_details,
        }

    def reset(self):
        """é‡ç½®è¨ˆæ•¸å™¨"""
        with self._lock:
            self.total_input_tokens = 0
            self.total_output_tokens = 0
            self.api_calls = 0
            self.call_details = []

    def print_summary(self):
        """åˆ—å°ä½¿ç”¨æ‘˜è¦"""
        summary = self.get_summary()
        print(f"\nğŸ“Š Tokenä½¿ç”¨æ‘˜è¦ï¼š")
        print(f"APIå‘¼å«æ¬¡æ•¸ï¼š{summary['total_api_calls']}")
        print(f"è¼¸å…¥Tokenï¼š{summary['total_input_tokens']:,}")
        print(f"è¼¸å‡ºTokenï¼š{summary['total_output_tokens']:,}")
        print(f"ç¸½è¨ˆTokenï¼š{summary['total_tokens']:,}")

        if summary["call_details"]:
            print(f"\nè©³ç´°å‘¼å«è¨˜éŒ„ï¼š")
            for i, detail in enumerate(summary["call_details"], 1):
                print(
                    f"  {i}. {detail['call_type']}: è¼¸å…¥={detail['input_tokens']:,}, è¼¸å‡º={detail['output_tokens']:,}, ç¸½è¨ˆ={detail['total_tokens']:,}"
                )


# å…¨å±€tokenè¿½è¹¤å™¨
token_tracker = TokenUsageTracker()

# æ·»åŠ  PDF_DIR å’Œ model_prompt_mapping ä»¥ä¾¿ GUI ä½¿ç”¨
PDF_DIR = Path(__file__).parent.parent / "assets/pdfs"
RESULTS_DIR = Path(__file__).parent.parent / "assets/results"
REPORTS_DIR = Path(__file__).parent.parent / "assets/reports"
TEMPLATE_PATH = Path(__file__).parent.parent / "assets/template.xlsx"

model_prompt_mapping = {
    "cash_equivalents": {
        "prompt": cash_equivalents_prompt,
        "model": CashAndEquivalents,
    },
    "total_liabilities": {
        "prompt": total_liabilities_prompt,
        "model": TotalLiabilities,
    },
    "prepayments": {
        "prompt": prepayments_prompt,
        "model": PrePayments,
    },
    "receivables_related_parties": {
        "prompt": receivables_related_parties_prompt,
        "model": ReceivablesRelatedParties,
    },
    "corporate_bond_payable": {
        "prompt": corporate_bond_payable_prompt,
        "model": CorporateBondPayable,
    },
    "property_plant_equipment": {
        "prompt": property_plant_equipment_prompt,
        "model": PropertyPlantEquipment,
    },
    "short_term_notes_payable": {
        "prompt": short_term_notes_payable_prompt,
        "model": ShortTermNotesPayable,
    },
}


# è²¡å‹™å ±è¡¨ä½ç½®æ¨¡å‹
class FinancialStatementLocation(BaseModel):
    """è²¡å‹™å ±è¡¨é …ç›®ä½ç½®"""

    item_name: str = Field(description="è²¡å‹™å ±è¡¨é …ç›®åç¨±")
    page_numbers: list[int] = Field(description="è©²é …ç›®æ‰€åœ¨çš„é æ•¸åˆ—è¡¨")
    found: bool = Field(description="æ˜¯å¦æ‰¾åˆ°è©²é …ç›®")


class FinancialStatementsAnalysis(BaseModel):
    """è²¡å‹™å ±è¡¨åˆ†æçµæœ"""

    individual_balance_sheet: FinancialStatementLocation = Field(
        description="å€‹é«”è³‡ç”¢è² å‚µè¡¨"
    )
    individual_comprehensive_income: FinancialStatementLocation = Field(
        description="å€‹é«”ç¶œåˆæç›Šè¡¨"
    )
    individual_equity_changes: FinancialStatementLocation = Field(
        description="å€‹é«”æ¬Šç›Šè®Šå‹•è¡¨"
    )
    individual_cash_flow: FinancialStatementLocation = Field(
        description="å€‹é«”ç¾é‡‘æµé‡è¡¨"
    )
    important_accounting_items: FinancialStatementLocation = Field(
        description="é‡è¦æœƒè¨ˆé …ç›®æ˜ç´°è¡¨"
    )

    def get_all_page_numbers(self) -> list[int]:
        """
        æå–æ‰€æœ‰è²¡å‹™å ±è¡¨çš„é ç¢¼

        å›å‚³ï¼š
            list[int]: æ’åºå¾Œçš„ä¸é‡è¤‡é ç¢¼åˆ—è¡¨
        """
        all_pages = []
        statements = [
            self.individual_balance_sheet,
            self.individual_comprehensive_income,
            self.individual_equity_changes,
            self.individual_cash_flow,
            self.important_accounting_items,
        ]

        for statement in statements:
            if statement.found and statement.page_numbers:
                all_pages.extend(statement.page_numbers)

        return sorted(list(set(all_pages)))


# æª¢æŸ¥æ˜¯å¦æ˜¯æƒææª”
def check_scanned_pages(
    pdf_path: str, pages_to_check: list[int] | None = None, text_threshold: int = 1
) -> dict[int, bool]:
    """
    æª¢æŸ¥æŒ‡å®šçš„é ç¢¼æ˜¯å¦ç‚ºã€Œæƒæå½±åƒé ã€(æ²’æœ‰å¯è¤‡è£½æ–‡å­—)ã€‚

    åƒæ•¸ï¼š
      - pdf_path (str)ï¼šPDF æª”æ¡ˆè·¯å¾‘ã€‚
      - pages_to_check (list[int])ï¼šè¦æª¢æŸ¥çš„é é¢åˆ—è¡¨ï¼ˆä»¥ 1 é–‹å§‹çš„é ç¢¼ï¼‰ã€‚
      - text_threshold (int)ï¼šåˆ¤æ–·æ–‡å­—é çš„é–€æª»ï¼ˆå­—å…ƒæ•¸ï¼‰ï¼Œé è¨­ 1ã€‚å¦‚æœå›å‚³çš„æ–‡å­—é•·åº¦ >= text_threshold å°±åˆ¤æ–·ç‚ºæœ‰æ–‡å­—å±¤ã€‚

    å›å‚³ï¼š
      - dict[int, bool]ï¼šä»¥é ç¢¼ç‚º keyï¼Œvalue ç‚ºå¸ƒæ—å€¼ã€‚å¦‚æœ True å‰‡ä»£è¡¨è©²é ã€Œç„¡å¯è¤‡è£½æ–‡å­—ã€ï¼Œæ¥µå¯èƒ½æ˜¯æƒæå½±åƒé ï¼›False ä»£è¡¨è©²é æœ‰æ–‡å­—å±¤ï¼Œå¯è¤‡è£½æ–‡å­—ã€‚
    """
    results: dict[int, bool] = {}
    doc = fitz.open(pdf_path)
    total_pages = doc.page_count

    if pages_to_check is None:
        pages_to_check = range(1, total_pages + 1)

    for pg in pages_to_check:
        # æª¢æŸ¥é ç¢¼æ˜¯å¦åˆç†
        if not (1 <= pg <= total_pages):
            raise ValueError(f"é ç¢¼ {pg} è¶…å‡ºç¯„åœï¼ˆ1~{total_pages}ï¼‰ã€‚")

        page = doc.load_page(pg - 1)  # PyMuPDF çš„é é¢ç´¢å¼•å¾ 0 é–‹å§‹
        text = page.get_text("text") or ""  # å–å‡ºç´”æ–‡å­—å±¤ï¼›å¦‚æœ Noneï¼Œå°±æ”¹æˆç©ºå­—ä¸²

        # æ ¹æ“š text_threshold åˆ¤æ–·æ˜¯å¦ã€Œç„¡å¯è¤‡è£½æ–‡å­—ã€
        if len(text.strip()) < text_threshold:
            # ç•¶æ–‡å­—é•·åº¦å°æ–¼é–€æª»ï¼Œå°±è¦–ç‚ºã€Œæƒæå½±åƒé ã€
            results[pg] = True
        else:
            # æœ‰è¶³å¤ æ–‡å­—ï¼Œå°±è¦–ç‚ºã€Œæ–‡å­—é ã€
            results[pg] = False

    doc.close()
    return results


def analyze_toc_and_extract_financial_statements(
    pdf_path: str,
) -> FinancialStatementsAnalysis:
    """
    åˆ†æPDFå‰5é çš„ç›®éŒ„å…§å®¹ï¼Œæ‰¾å‡ºå„è²¡å‹™å ±è¡¨é …ç›®çš„é æ•¸ä½ç½®

    åƒæ•¸ï¼š
        pdf_path (str): PDFæª”æ¡ˆè·¯å¾‘

    å›å‚³ï¼š
        FinancialStatementsAnalysis: åŒ…å«å„è²¡å‹™å ±è¡¨ä½ç½®çš„åˆ†æçµæœ
    """
    try:
        # é–‹å•ŸPDFæ–‡ä»¶
        doc = fitz.open(pdf_path)
        total_pages = doc.page_count

        print(f"æ­£åœ¨åˆ†æPDFæ–‡ä»¶ï¼š{pdf_path}")
        print(f"ç¸½é æ•¸ï¼š{total_pages}")

        # æå–å‰5é ï¼ˆæˆ–å…¨éƒ¨é é¢ï¼Œå¦‚æœå°‘æ–¼5é ï¼‰
        pages_to_extract = min(5, total_pages)
        print(f"æå–å‰ {pages_to_extract} é é€²è¡Œç›®éŒ„åˆ†æ...")

        # å‰µå»ºåŒ…å«å‰5é çš„æ–°PDF
        pdf_writer = fitz.open()  # å‰µå»ºæ–°çš„PDFæ–‡æª”

        for page_num in range(pages_to_extract):
            page = doc.load_page(page_num)
            pdf_writer.insert_pdf(doc, from_page=page_num, to_page=page_num)

        # å°‡å‰5é è½‰æ›ç‚ºbase64
        pdf_bytes = pdf_writer.tobytes()
        base64_content = base64.b64encode(pdf_bytes).decode("utf-8")

        # é—œé–‰æ–‡æª”
        doc.close()
        pdf_writer.close()

        # æº–å‚™åˆ†æç›®éŒ„çš„æç¤ºè©
        prompt = """
        è«‹åˆ†æç›®éŒ„é ï¼Œæ‰¾å‡ºä»¥ä¸‹è²¡å‹™å ±è¡¨é …ç›®åœ¨ç›®éŒ„ä¸­é¡¯ç¤ºçš„é æ•¸ï¼š

        1. å€‹é«”è³‡ç”¢è² å‚µè¡¨
        2. å€‹é«”ç¶œåˆæç›Šè¡¨  
        3. å€‹é«”æ¬Šç›Šè®Šå‹•è¡¨
        4. å€‹é«”ç¾é‡‘æµé‡è¡¨
        5. é‡è¦æœƒè¨ˆé …ç›®æ˜ç´°è¡¨

        è«‹ä»”ç´°æŸ¥çœ‹ç›®éŒ„ä¸­çš„é …ç›®åç¨±ï¼Œå¯èƒ½æœƒæœ‰é¡ä¼¼çš„è¡¨é”æ–¹å¼ï¼Œä¾‹å¦‚ï¼š
        - "è³‡ç”¢è² å‚µè¡¨"ã€"Balance Sheet"
        - "ç¶œåˆæç›Šè¡¨"ã€"Comprehensive Income Statement"
        - "æ¬Šç›Šè®Šå‹•è¡¨"ã€"Statement of Changes in Equity"
        - "ç¾é‡‘æµé‡è¡¨"ã€"Cash Flow Statement"
        - "é‡è¦æœƒè¨ˆé …ç›®æ˜ç´°è¡¨"ã€"Notes to Financial Statements"ã€"é™„è¨»"

        æ³¨æ„ï¼š
        - è«‹æ ¹æ“šç›®éŒ„ä¸­é¡¯ç¤ºçš„é æ•¸å¡«å¯«
        - å¦‚æœæ‰¾ä¸åˆ°æŸå€‹é …ç›®ï¼Œè«‹å°‡foundè¨­ç‚ºfalse
        - å¦‚æœæŸå€‹å ±è¡¨è·¨è¶Šå¤šé ï¼Œè«‹åˆ—å‡ºæ‰€æœ‰ç›¸é—œé æ•¸
        - é‡è¦æœƒè¨ˆé …ç›®æ˜ç´°è¡¨ä¸ç­‰æ–¼é‡è¦æœƒè¨ˆé …ç›®ä¹‹èªªæ˜ï¼Œè«‹æ³¨æ„å€åˆ†
        """

        print("æ­£åœ¨åˆ†æç›®éŒ„é å…§å®¹...")

        result = call_gemini(
            prompt, base64_content, FinancialStatementsAnalysis, "ç›®éŒ„åˆ†æ"
        )
        print("ç›®éŒ„åˆ†æå®Œæˆï¼")

        return result

    except Exception as e:
        print(f"åˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
        raise e


def convert_pdf_to_markdown(
    pdf_path: str, pages: list[int], max_workers: int = 4
) -> dict[int, str]:
    """
    å°‡æŒ‡å®šé æ•¸çš„PDFè½‰æ›ç‚ºMarkdownæ ¼å¼ï¼ˆä½¿ç”¨å¤šç·šç¨‹ä¸¦è¡Œè™•ç†ï¼‰

    åƒæ•¸ï¼š
        pdf_path (str): PDFæª”æ¡ˆè·¯å¾‘
        pages (list[int]): è¦è½‰æ›çš„é æ•¸åˆ—è¡¨ï¼ˆ1-basedé ç¢¼ï¼‰
        max_workers (int): æœ€å¤§ç·šç¨‹æ•¸ï¼Œé è¨­ç‚º4

    å›å‚³ï¼š
        dict[int, str]: ä»¥é ç¢¼ç‚ºkeyï¼Œvalueç‚ºè½‰æ›å¾Œçš„Markdownå…§å®¹
    """
    try:
        # é–‹å•ŸPDFæ–‡ä»¶
        doc = fitz.open(pdf_path)
        total_pages = doc.page_count

        print(f"æ­£åœ¨è½‰æ›PDFé é¢ {pages} ç‚ºMarkdownæ ¼å¼")
        valid_pages = [p for p in pages if 1 <= p <= total_pages]
        if not valid_pages:
            doc.close()
            raise ValueError(f"æ²’æœ‰æœ‰æ•ˆçš„é ç¢¼ã€‚æœ‰æ•ˆç¯„åœï¼š1-{total_pages}")

        markdown_content = {}
        lock = threading.Lock()

        def process_single_page(page: int) -> tuple[int, str]:
            """è™•ç†å–®å€‹é é¢çš„è½‰æ›"""
            try:
                # ç‚ºæ¯å€‹é é¢å‰µå»ºç¨ç«‹çš„PDFæ–‡æª”
                with lock:  # ä¿è­·PDFæ–‡æª”æ“ä½œ
                    single_page_doc = fitz.open()
                    single_page_doc.insert_pdf(
                        doc, from_page=page - 1, to_page=page - 1
                    )
                    pdf_bytes = single_page_doc.tobytes()
                    single_page_doc.close()

                # å°‡å–®é è½‰æ›ç‚ºbase64
                base64_content = base64.b64encode(pdf_bytes).decode("utf-8")

                # æº–å‚™è½‰æ›ç‚ºMarkdownçš„æç¤ºè©
                prompt = """
                è«‹å°‡é€™å€‹PDFæ–‡ä»¶çš„å…§å®¹è½‰æ›ç‚ºMarkdownæ ¼å¼ã€‚

                è½‰æ›è¦æ±‚ï¼š
                1. ä¿æŒè¡¨æ ¼çµæ§‹ï¼Œä½¿ç”¨Markdownè¡¨æ ¼èªæ³•
                2. ä¿æŒæ¨™é¡Œå±¤ç´šçµæ§‹
                3. ä¿æŒæ•¸å­—å’Œæ–‡å­—çš„ç²¾ç¢ºæ€§
                4. å¦‚æœæœ‰è²¡å‹™æ•¸æ“šè¡¨æ ¼ï¼Œè«‹ç¢ºä¿æ•¸å­—å°é½Šå’Œæ ¼å¼æ­£ç¢º
                5. ä¿æŒåŸå§‹çš„æ®µè½åˆ†éš”

                è«‹ç›´æ¥è¿”å›Markdownæ ¼å¼çš„æ–‡æœ¬å…§å®¹ï¼Œä¸éœ€è¦é¡å¤–çš„æ ¼å¼åŒ…è£ã€‚
                """

                print(f"æ­£åœ¨å‘¼å«Gemini APIè½‰æ›ç¬¬ {page} é ...")

                result = call_gemini(
                    prompt, base64_content, None, f"PDFè½‰Markdown_ç¬¬{page}é "
                )

                print(f"ç¬¬ {page} é è½‰æ›å®Œæˆ")
                return page, result

            except Exception as page_error:
                print(f"è½‰æ›ç¬¬ {page} é æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{page_error}")
                error_content = f"# ç¬¬ {page} é è½‰æ›å¤±æ•—\n\nè½‰æ›éŒ¯èª¤ï¼š{str(page_error)}"
                return page, error_content

        # ä½¿ç”¨ThreadPoolExecutoré€²è¡Œä¸¦è¡Œè™•ç†
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»å‹™
            future_to_page = {
                executor.submit(process_single_page, page): page for page in valid_pages
            }

            # æ”¶é›†çµæœ
            for future in as_completed(future_to_page):
                page_num, content = future.result()
                markdown_content[page_num] = content

        # é—œé–‰åŸå§‹æ–‡æª”
        doc.close()

        print(f"PDFè½‰Markdownå®Œæˆï¼æˆåŠŸè½‰æ›äº† {len(markdown_content)} é ")
        return markdown_content

    except Exception as e:
        print(f"è½‰æ›PDFç‚ºMarkdownæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
        # ç¢ºä¿æ–‡æª”è¢«é—œé–‰
        try:
            if "doc" in locals():
                doc.close()
        except:
            pass
        raise e


@contextmanager
def temporary_files(*suffixes):
    """
    ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼šå‰µå»ºå¤šå€‹è‡¨æ™‚æª”æ¡ˆï¼Œè‡ªå‹•æ¸…ç†

    åƒæ•¸ï¼š
        *suffixes: æª”æ¡ˆå¾Œç¶´ååˆ—è¡¨

    ç”¢å‡ºï¼š
        list[str]: è‡¨æ™‚æª”æ¡ˆè·¯å¾‘åˆ—è¡¨
    """
    temp_files = []
    try:
        for suffix in suffixes:
            temp_file = tempfile.NamedTemporaryFile(suffix=suffix, delete=False)
            temp_files.append(temp_file.name)
            temp_file.close()
        yield temp_files
    finally:
        # æ¸…ç†æ‰€æœ‰è‡¨æ™‚æª”æ¡ˆ
        for temp_path in temp_files:
            try:
                os.unlink(temp_path)
            except (OSError, FileNotFoundError):
                pass  # æª”æ¡ˆå¯èƒ½å·²è¢«åˆªé™¤æˆ–ä¸å­˜åœ¨


def convert_markdown_to_pdf(markdown_content: dict[int, str], pdf_path: str) -> str:
    """
    å°‡Markdownå…§å®¹è½‰æ›ç‚ºPDFé é¢ï¼Œä¸¦æ›¿æ›åŸå§‹PDFä¸­çš„å°æ‡‰é é¢

    åƒæ•¸ï¼š
        markdown_content (dict[int, str]): ä»¥é ç¢¼ç‚ºkeyï¼Œvalueç‚ºMarkdownå…§å®¹
        pdf_path (str): åŸå§‹PDFæª”æ¡ˆè·¯å¾‘

    å›å‚³ï¼š
        str: æ–°PDFæª”æ¡ˆçš„è·¯å¾‘
    """
    try:
        if not markdown_content:
            print("æ²’æœ‰Markdownå…§å®¹éœ€è¦è½‰æ›")
            return pdf_path

        print(f"æ­£åœ¨ä½¿ç”¨ Spire.Doc å°‡Markdownå…§å®¹è½‰æ›ç‚ºPDFä¸¦æ›¿æ›æƒæé é¢...")

        # é–‹å•ŸåŸå§‹PDF
        original_doc = fitz.open(pdf_path)
        total_pages = original_doc.page_count

        # å‰µå»ºæ–°çš„PDFæ–‡æª”
        new_doc = fitz.open()
        from spire.doc import Document, FileFormat

        try:
            # é€é è™•ç†
            for page_num in range(1, total_pages + 1):
                if page_num in markdown_content:
                    print(f"æ­£åœ¨è½‰æ›ç¬¬ {page_num} é çš„Markdownå…§å®¹...")

                    # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨è™•ç†è‡¨æ™‚æª”æ¡ˆ
                    with temporary_files(".md", ".pdf") as (
                        temp_md_path,
                        temp_pdf_path,
                    ):
                        # å¯«å…¥Markdownå…§å®¹
                        with open(temp_md_path, "w", encoding="utf-8") as f:
                            f.write(markdown_content[page_num])

                        # å‰µå»ºWordæ–‡æª”ä¸¦è½‰æ›
                        word_doc = Document()
                        try:
                            word_doc.LoadFromFile(temp_md_path)
                            word_doc.SaveToFile(temp_pdf_path, FileFormat.PDF)
                        finally:
                            word_doc.Dispose()

                        # è®€å–ç”Ÿæˆçš„PDFä¸¦æ’å…¥åˆ°æ–°æ–‡æª”ä¸­
                        converted_pdf = fitz.open(temp_pdf_path)
                        try:
                            new_doc.insert_pdf(converted_pdf)
                        finally:
                            converted_pdf.close()

                    print(f"ç¬¬ {page_num} é è½‰æ›å®Œæˆ")

                else:
                    # ä¿ç•™åŸå§‹é é¢
                    new_doc.insert_pdf(
                        original_doc, from_page=page_num - 1, to_page=page_num - 1
                    )

            # ç”Ÿæˆæ–°çš„æª”æ¡ˆåç¨±
            original_path = Path(pdf_path)
            new_pdf_path = (
                original_path.parent
                / f"{original_path.stem}_converted{original_path.suffix}"
            )

            # ä¿å­˜æ–°çš„PDF
            new_doc.save(str(new_pdf_path))
            return str(new_pdf_path)

        finally:
            # ç¢ºä¿æ–‡æª”è¢«é—œé–‰
            new_doc.close()
            original_doc.close()

    except Exception as e:
        print(f"è½‰æ›Markdownç‚ºPDFæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
        import sys

        sys.exit(1)


def genetate_verification_report(
    results: dict[str, BaseModel], pdf_data, filepath: Path
) -> str:
    # å°‡resultsè½‰æ›ç‚ºæ˜“è®€çš„æ–‡å­—æ ¼å¼
    results_text = "\n\n## å·²æå–çš„è²¡å‹™æ•¸æ“šï¼š\n"
    for model_name, data in results.items():
        results_text += f"\n### {model_name}:\n"
        if data:
            results_text += data.model_dump_json()
        else:
            results_text += "æå–å¤±æ•—æˆ–ç„¡æ•¸æ“š"
        results_text += "\n"

    # æª¢æŸ¥å’Œé©—è­‰results
    prompt = f"""
    ä½ æ˜¯å°ˆæ¥­çš„è²¡å‹™åˆ†æå¸«ï¼Œè«‹åˆ†æé€™å€‹è²¡å‹™å ±è¡¨PDFæ–‡ä»¶ï¼Œä¸¦æ ¹æ“šå·²æå–çš„æ•¸æ“šé€²è¡Œé©—è­‰å’Œè£œå……åˆ†æã€‚

    ## ä»»å‹™è¦æ±‚ï¼š
    1. **é©—è­‰å·²æå–æ•¸æ“šçš„æº–ç¢ºæ€§**ï¼šæª¢æŸ¥ä¸‹æ–¹æå–çš„è²¡å‹™æ•¸æ“šæ˜¯å¦èˆ‡PDFä¸­çš„åŸå§‹æ•¸æ“šä¸€è‡´
    2. **æ‰¾å‡ºéºæ¼æˆ–éŒ¯èª¤çš„é …ç›®**ï¼šè­˜åˆ¥å¯èƒ½è¢«éºæ¼æˆ–æå–éŒ¯èª¤çš„è²¡å‹™æ•¸æ“š
    3. **æ•¸æ“šä¸€è‡´æ€§æª¢æŸ¥**ï¼šç¢ºèªå„è²¡å‹™å ±è¡¨é …ç›®ä¹‹é–“çš„é‚è¼¯ä¸€è‡´æ€§

    ## åˆ†æé‡é»ï¼š
    - ç¾é‡‘åŠç´„ç•¶ç¾é‡‘é …ç›®çš„å®Œæ•´æ€§å’Œæº–ç¢ºæ€§
    - é ä»˜æ¬¾é …çš„åˆ†é¡å’Œé‡‘é¡æ­£ç¢ºæ€§  
    - é—œä¿‚äººæ‡‰æ”¶æ¬¾é …çš„è©³ç´°åˆ†æ
    - è² å‚µç¸½é¡çš„è¨ˆç®—å’Œåˆ†é¡æ­£ç¢ºæ€§

    ## åƒè€ƒæ¨™æº–ï¼š
    - ç¢ºä¿æ•¸å­—ç²¾ç¢ºåˆ°å°æ•¸é»
    - æ³¨æ„è²¨å¹£å–®ä½ï¼ˆåƒå…ƒã€è¬å…ƒç­‰ï¼‰
    - æª¢æŸ¥æœŸé–“æ¯”è¼ƒæ•¸æ“šçš„ä¸€è‡´æ€§
    - é©—è­‰æœƒè¨ˆç§‘ç›®åˆ†é¡çš„æ­£ç¢ºæ€§
    - å„ªå…ˆä»¥å¤§è¡¨çš„æ•¸æ“šç‚ºä¸»ï¼Œé™„è¨»çš„æ•¸æ“šç‚ºè¼”

    {results_text}

    ## è¼¸å‡ºæ ¼å¼ï¼š
    è«‹ä»¥Markdownçš„æ–¹å¼ä½¿ç”¨ç¹é«”ä¸­æ–‡å›æ‡‰ï¼ŒåŒ…å«ï¼š
    1. **æ•¸æ“šé©—è­‰çµæœ**ï¼šå·²æå–æ•¸æ“šçš„æº–ç¢ºæ€§è©•ä¼°
    2. **ç™¼ç¾çš„å•é¡Œ**ï¼šéŒ¯èª¤ã€éºæ¼æˆ–ä¸ä¸€è‡´ä¹‹è™•
    3. **è£œå……è³‡è¨Š**ï¼šPDFä¸­å…¶ä»–é‡è¦çš„è²¡å‹™è³‡è¨Š
    4. **å»ºè­°æ”¹é€²**ï¼šæ•¸æ“šæå–å¯ä»¥æ”¹é€²çš„åœ°æ–¹
    """

    # ä¿å­˜é©—è­‰çµæœ
    result = call_gemini(prompt, pdf_data, None, "é©—è­‰å ±å‘Šç”Ÿæˆ")

    # ç¢ºä¿å ±å‘Šç›®éŒ„å­˜åœ¨
    REPORTS_DIR.mkdir(parents=True, exist_ok=True)

    # ä½¿ç”¨ REPORTS_DIR ä¿å­˜é©—è­‰å ±å‘Š
    report_path = REPORTS_DIR / f"{filepath.stem}_verification.md"
    with open(report_path, "w", encoding="utf-8") as f:
        f.write(f"# {filepath.stem} è²¡å‹™æ•¸æ“šé©—è­‰å ±å‘Š\n\n")
        f.write(result)

    print(f"âœ“ è²¡å‹™æ•¸æ“šé©—è­‰å®Œæˆï¼Œå ±å‘Šå·²ä¿å­˜è‡³ï¼š{report_path}")
    return str(report_path)


def process_single_pdf_with_gemini(
    filepath: Path, model_selection: list[str]
) -> tuple[dict, str, dict]:
    """
    ä½¿ç”¨ Gemini è™•ç†å–®å€‹ PDF æª”æ¡ˆçš„è²¡å‹™å ±è¡¨åˆ†æ

    åƒæ•¸:
        filepath: PDF æª”æ¡ˆè·¯å¾‘
        model_selection: é¸æ“‡çš„æ¨¡å‹åˆ—è¡¨

    å›å‚³:
        tuple[dict, str, dict]: åŒ…å«æ‰€æœ‰æ¨¡å‹åˆ†æçµæœçš„å­—å…¸ã€é©—è­‰å ±å‘Šçš„è·¯å¾‘å’Œtokenä½¿ç”¨è³‡è¨Š
    """
    try:
        # è¨˜éŒ„è™•ç†é–‹å§‹æ™‚çš„tokenä½¿ç”¨é‡
        # start_tokens = token_tracker.get_summary()

        # åˆ†æç›®éŒ„ä¸¦æå–è²¡å‹™å ±è¡¨ä½ç½®
        toc_content = analyze_toc_and_extract_financial_statements(filepath)

        # æ­£ç¢ºåœ°æå–æ‰€æœ‰è²¡å‹™å ±è¡¨çš„é ç¢¼
        table_pages = toc_content.get_all_page_numbers()

        # æª¢æŸ¥æ˜¯å¦ç‚ºæƒææ–‡ä»¶
        scan_results = check_scanned_pages(filepath, table_pages) if table_pages else {}
        scanned_pages = [page for page, is_scan in scan_results.items() if is_scan]

        # è™•ç†æƒæé é¢
        if scanned_pages:
            try:
                markdown_content = convert_pdf_to_markdown(str(filepath), scanned_pages)
                new_pdf_path = convert_markdown_to_pdf(markdown_content, str(filepath))
                filepath = Path(new_pdf_path)
                print(f"å·²è™•ç†æƒæé é¢ä¸¦ç”Ÿæˆæ–°PDF: {new_pdf_path}")
            except Exception as e:
                print(f"è½‰æ›æƒæé é¢å¤±æ•—ï¼š{e}")
                # ç¹¼çºŒä½¿ç”¨åŸå§‹PDF

        # è®€å–PDFä¸¦è½‰æ›ç‚ºbase64
        pdf_data = base64.b64encode(filepath.read_bytes()).decode("utf-8")

        def process_model(prompt_model_pair) -> tuple[str, BaseModel | None]:
            """è™•ç†å–®å€‹æ¨¡å‹çš„åˆ†æ"""
            model, prompt = prompt_model_pair
            prompt = prompt + f"\n\nè³‡æ–™è«‹å„ªå…ˆä»¥å¤§è¡¨ç‚ºä¸»ï¼Œé™„è¨»ç‚ºè¼”"
            try:
                result: BaseModel = call_gemini(
                    prompt, pdf_data, model, f"è²¡å‹™åˆ†æ_{model.__name__}"
                )
                # ç›´æ¥è¿”å›æ¨¡å‹å°è±¡è€Œä¸æ˜¯å­—å…¸
                return model.__name__, result
            except Exception as e:
                print(f"{model.__name__} åˆ†æå¤±æ•—ï¼š{e}")
                return model.__name__, None

        # model_prompt_mapping
        if model_selection:
            model_pairs = [
                (
                    model_prompt_mapping[model_name]["model"],
                    model_prompt_mapping[model_name]["prompt"],
                )
                for model_name in model_prompt_mapping.keys()
                if model_name in model_selection
            ]
        else:
            model_pairs = [
                (
                    model_prompt_mapping[model_name]["model"],
                    model_prompt_mapping[model_name]["prompt"],
                )
                for model_name in model_prompt_mapping.keys()
            ]
        results: dict[str, BaseModel] = {}
        # ä½¿ç”¨ThreadPoolExecutoré€²è¡Œä¸¦è¡Œè™•ç†
        with ThreadPoolExecutor(max_workers=4) as executor:
            # æäº¤æ‰€æœ‰ä»»å‹™
            future_to_model = {
                executor.submit(process_model, pair): pair[0].__name__
                for pair in model_pairs
            }

            # æ”¶é›†çµæœ
            for future in as_completed(future_to_model):
                model_name = future_to_model[future]
                try:
                    result_name, result_data = future.result()
                    if result_data is not None:
                        results[result_name] = result_data
                        print(f"âœ“ {model_name} è™•ç†å®Œæˆ")
                    else:
                        print(f"âœ— {model_name} è™•ç†å¤±æ•—")
                except Exception as e:
                    print(f"âœ— {model_name} è™•ç†ç•°å¸¸ï¼š{e}")

        verification_report_path = genetate_verification_report(
            results, pdf_data, filepath
        )
        verification_report_path = ""

        # è¨ˆç®—æœ¬æ¬¡è™•ç†ä½¿ç”¨çš„token
        # end_tokens = token_tracker.get_summary()
        # process_tokens = {
        #     "input_tokens": end_tokens["total_input_tokens"]
        #     - start_tokens["total_input_tokens"],
        #     "output_tokens": end_tokens["total_output_tokens"]
        #     - start_tokens["total_output_tokens"],
        #     "total_tokens": (
        #         end_tokens["total_input_tokens"] + end_tokens["total_output_tokens"]
        #     )
        #     - (
        #         start_tokens["total_input_tokens"] + start_tokens["total_output_tokens"]
        #     ),
        #     "api_calls": end_tokens["total_api_calls"]
        #     - start_tokens["total_api_calls"],
        # }

        return results, verification_report_path, process_tokens

    except Exception as e:
        print(f"è™•ç† PDF æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        return {}, "", {}


def export_excel(results: dict[str, BaseModel], filepath: Path):
    # æª¢æŸ¥æ¨¡æ¿æª”æ¡ˆçš„æ ¼å¼
    # if TEMPLATE_PATH.suffix.lower() == ".xls":
    #     # è™•ç† .xls æ ¼å¼
    #     import pandas as pd
    #     from openpyxl import Workbook

    #     # ä½¿ç”¨ pandas è®€å– .xls æª”æ¡ˆçš„æ‰€æœ‰å·¥ä½œè¡¨
    #     df_dict = pd.read_excel(TEMPLATE_PATH, sheet_name=None)

    #     # å‰µå»ºæ–°çš„ openpyxl å·¥ä½œç°¿
    #     wb = Workbook()
    #     # ç§»é™¤é»˜èªçš„å·¥ä½œè¡¨
    #     wb.remove(wb.active)

    #     # å°‡æ¯å€‹å·¥ä½œè¡¨è½‰æ›ç‚º openpyxl æ ¼å¼
    #     for sheet_name, df in df_dict.items():
    #         ws = wb.create_sheet(title=sheet_name)
    #         # å°‡ DataFrame å¯«å…¥å·¥ä½œè¡¨
    #         for row_idx, row in enumerate(df.values, 1):
    #             for col_idx, value in enumerate(row, 1):
    #                 ws.cell(
    #                     row=row_idx + 1, column=col_idx, value=value
    #                 )  # +1 å› ç‚ºè¦ä¿ç•™æ¨™é¡Œè¡Œ
    #         # å¯«å…¥æ¨™é¡Œè¡Œ
    #         for col_idx, col_name in enumerate(df.columns, 1):
    #             ws.cell(row=1, column=col_idx, value=col_name)
    # else:
    #     # è™•ç† .xlsx æ ¼å¼ï¼ˆåŸæœ‰é‚è¼¯ï¼‰
    #     wb = load_workbook(TEMPLATE_PATH, keep_vba=True)

    wb = load_workbook(TEMPLATE_PATH)

    for model in results.values():
        # æŠŠåŒä¸€å€‹ worksheet å‚³çµ¦æ¯å€‹ model å¯«å…¥
        model.fill_excel(wb)

    # å…¨éƒ¨å¡«å®Œå¾Œå†å­˜æª”
    wb.save("output.xlsx")


# å¦‚æœç›´æ¥åŸ·è¡Œæ­¤æª”æ¡ˆï¼Œé‹è¡Œæ¸¬è©¦
if __name__ == "__main__":
    # é‡ç½®tokenè¨ˆæ•¸å™¨
    # token_tracker.reset()

    # æ¸¬è©¦ç”¨çš„ PDF æª”æ¡ˆ
    test_files = [
        "quartely-results-2024-zh_tcm27-94407.pdf",
    ]

    for filename in test_files:
        filepath = PDF_DIR / filename
        if filepath.exists():
            print(f"æ¸¬è©¦è™•ç†æª”æ¡ˆ: {filename}")
            results, verification_report_path, process_tokens = (
                process_single_pdf_with_gemini(
                    filepath, model_selection=model_prompt_mapping.keys()
                )
            )
            export_excel(results, "")
            # ç¢ºä¿çµæœç›®éŒ„å­˜åœ¨
            RESULTS_DIR.mkdir(parents=True, exist_ok=True)

            # ä½¿ç”¨ RESULTS_DIR ä¿å­˜çµæœ
            results_path = RESULTS_DIR / f"{filepath.stem}_gemini_results.json"
            with open(results_path, "w", encoding="utf-8") as f:
                json_results = {}
                for model_name, result in results.items():
                    if hasattr(result, "model_dump"):
                        json_results[model_name] = result.model_dump()
                    else:
                        json_results[model_name] = str(result)
                json.dump(json_results, f, indent=4, ensure_ascii=False)

            print(f"âœ“ çµæœå·²ä¿å­˜åˆ°: {results_path}")

            # é¡¯ç¤ºæœ¬æ¬¡æª”æ¡ˆè™•ç†çš„tokenä½¿ç”¨æƒ…æ³
            print(f"ğŸ“Š æœ¬æ¬¡è™•ç† '{filename}' çš„Tokenä½¿ç”¨ï¼š")
            print(f"   APIå‘¼å«ï¼š{process_tokens.get('api_calls', 0)} æ¬¡")
            print(f"   è¼¸å…¥Tokenï¼š{process_tokens.get('input_tokens', 0):,}")
            print(f"   è¼¸å‡ºTokenï¼š{process_tokens.get('output_tokens', 0):,}")
            print(f"   ç¸½è¨ˆTokenï¼š{process_tokens.get('total_tokens', 0):,}")
        else:
            print(f"âœ— æª”æ¡ˆä¸å­˜åœ¨: {filepath}")

    # é¡¯ç¤ºtokenä½¿ç”¨æ‘˜è¦
    # token_tracker.print_summary()
